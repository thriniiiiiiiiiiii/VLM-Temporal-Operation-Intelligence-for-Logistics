# ── Model Configuration ───────────────────────────────────────────────────────
model:
  base_id: "Qwen/Qwen2.5-VL-3B-Instruct"
  cache_dir: "/root/.cache/huggingface"
  trust_remote_code: true

# ── Quantization ──────────────────────────────────────────────────────────────
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"           # NormalFloat4 — optimal for LLM weights
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true      # Nested quantization saves ~0.35 bits/param

# ── LoRA Configuration ────────────────────────────────────────────────────────
lora:
  r: 16                                 # Rank: balance of capacity vs param budget
  lora_alpha: 32                        # Alpha = 2×r is the standard scaling
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# ── Training Hyperparameters ──────────────────────────────────────────────────
training:
  output_dir: "./checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8       # Effective batch = 2×8 = 16
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  fp16: true
  bf16: false
  gradient_checkpointing: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  # Checkpoint strategy — Kaggle sessions die after 12h
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  evaluation_strategy: "steps"
  eval_steps: 100
  logging_steps: 10
  load_best_model_at_end: false
  metric_for_best_model: "eval_loss"
  report_to: "wandb"
  run_name: "vlm-openpack-qwen25-qloraV1"
  # Resume on crash
  resume_from_checkpoint: true
  optim: "adamw_bnb_8bit"

# ── Data Configuration ────────────────────────────────────────────────────────
data:
  openpack_root: "mock_data"
  train_subjects: ["U0101", "U0102", "U0103", "U0104", "U0105", "U0106"]
  val_subjects: ["U0107"]
  test_subjects: ["U0108"]
  frames_per_clip: 8
  clip_duration_seconds: 5.0
  fps: 25
  frame_size: 336                       # Qwen2.5-VL native resolution
  boundary_window_seconds: 0.5         # ±0.5s around operation transitions
  shard_size_mb: 200
  shard_dir: "mock_shards"
  samples_dir: "./training_data_samples"
  num_sample_clips: 20

# ── VRAM Budget Verification ──────────────────────────────────────────────────
# model_base_4bit  = 3.0 GB
# lora_adapters    = 0.3 GB
# frames_per_clip  = 8
# frame_tokens     = 256
# batch_size       = 2
# hidden_dim       = 2048
# activation_gb    = (8 × 256 × 2 × 2048 × 2) / 1e9 = 0.0167 GB
# grad_ckpt_factor = 0.4  → 0.006 GB
# optimizer_gb     = 0.08 GB (AdamW states for LoRA params only)
# TOTAL ESTIMATE   ≈ 3.39 GB base + overhead ≈ 8-10 GB observed
# T4 limit         = 16 GB → SAFE

# ── Inference Configuration ───────────────────────────────────────────────────
inference:
  max_new_tokens: 256
  temperature: 0.1
  do_sample: false
  top_p: 0.9
  repetition_penalty: 1.1

# ── Operation Classes ─────────────────────────────────────────────────────────
operations:
  classes:
    - "Box Setup"
    - "Inner Packing"
    - "Tape"
    - "Put Items"
    - "Pack"
    - "Wrap"
    - "Label"
    - "Final Check"
    - "Idle"
    - "Unknown"
  # Workflow adjacency — used for AA@1 evaluation + soft supervision
  transitions:
    "Box Setup": ["Inner Packing", "Idle"]
    "Inner Packing": ["Put Items", "Idle"]
    "Put Items": ["Pack", "Tape", "Idle"]
    "Pack": ["Tape", "Wrap", "Idle"]
    "Tape": ["Put Items", "Label", "Pack", "Idle"]
    "Wrap": ["Label", "Idle"]
    "Label": ["Final Check", "Idle"]
    "Final Check": ["Idle", "Box Setup"]
    "Idle": ["Box Setup", "Inner Packing", "Tape", "Put Items",
             "Pack", "Wrap", "Label", "Final Check"]
