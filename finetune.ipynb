{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2.5-VL-2B QLoRA Fine-tuning — OpenPack Temporal Operations\n",
    "\n",
    "**[REPLACE WITH YOUR KAGGLE NOTEBOOK PUBLIC URL]**\n",
    "https://www.kaggle.com/code/YOUR_USERNAME/vlm-openpack-finetune\n",
    "\n",
    "Assignment: VLM Challenge — Temporal Operation Intelligence for Logistics  \n",
    "Dataset: OpenPack (U0101–U0106 train, U0107 val, U0108 test)  \n",
    "Model: Qwen2.5-VL-2B-Instruct + 4-bit QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Environment Check ─────────────────────────────────────────────────────────\n",
    "import subprocess, torch\n",
    "\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'GPU count:      {torch.cuda.device_count()}')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    p = torch.cuda.get_device_properties(i)\n",
    "    print(f'  GPU {i}: {p.name} | {p.total_memory / 1e9:.1f} GB')\n",
    "\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free',\n",
    "                         '--format=csv'], capture_output=True, text=True)\n",
    "print('\\n' + result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Install Dependencies ──────────────────────────────────────────────────────\n",
    "!pip install -q transformers==4.45.2 peft==0.10.0 bitsandbytes==0.43.1 \\\n",
    "             accelerate==0.30.1 trl==0.8.6 qwen-vl-utils==0.0.8 \\\n",
    "             webdataset==0.2.86 decord==0.6.0 loguru==0.7.2 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ╔══════════════════════════════════════════════════════════╗\n",
    "# ║           VRAM BUDGET CALCULATION (REQUIRED CELL)       ║\n",
    "# ╚══════════════════════════════════════════════════════════╝\n",
    "\n",
    "model_base_4bit   = 2.0    # GB — Qwen2.5-VL-2B at 4-bit NF4\n",
    "lora_adapters     = 0.3    # GB — LoRA r=16, ~10M trainable params\n",
    "frames_per_clip   = 8      # Sampled frames per 5-sec clip\n",
    "frame_tokens      = 256    # Visual tokens per frame (vision encoder output)\n",
    "batch_size        = 2      # Per-device batch size\n",
    "token_hidden_dim  = 1536   # Qwen2.5-VL-2B hidden dimension\n",
    "\n",
    "activation_gb = (frames_per_clip * frame_tokens * batch_size * token_hidden_dim * 2) / 1e9\n",
    "# With gradient checkpointing: activations recomputed during backward, not stored\n",
    "activation_gc = activation_gb * 0.4   # 0.4 = fraction actually stored\n",
    "\n",
    "# AdamW optimizer states for LoRA params only (base model frozen)\n",
    "optimizer_gb = (lora_adapters * 1e9 * 2 * 4) / 1e9  # 2 states × 4 bytes\n",
    "\n",
    "theoretical_min = model_base_4bit + lora_adapters + activation_gc + optimizer_gb\n",
    "cuda_overhead   = 6.0   # KV cache + CUDA context + framework overhead\n",
    "total_observed  = theoretical_min + cuda_overhead\n",
    "\n",
    "print('═' * 55)\n",
    "print('  VRAM BUDGET CALCULATION')\n",
    "print('═' * 55)\n",
    "print(f'  model_base_4bit   = {model_base_4bit:.3f} GB')\n",
    "print(f'  lora_adapters     = {lora_adapters:.3f} GB')\n",
    "print(f'  activation_raw    = {activation_gb:.4f} GB')\n",
    "print(f'  activation_gc     = {activation_gc:.4f} GB  (×0.4 factor)')\n",
    "print(f'  optimizer_adamw   = {optimizer_gb:.4f} GB')\n",
    "print(f'  ─────────────────────────────────────')\n",
    "print(f'  Theoretical min   = {theoretical_min:.3f} GB')\n",
    "print(f'  + CUDA overhead   = {cuda_overhead:.1f}   GB (empirical)')\n",
    "print(f'  Expected observed = {total_observed:.1f} GB')\n",
    "print(f'  T4 VRAM limit     = 16.0 GB')\n",
    "safe = '✓ SAFE' if total_observed < 16 else '✗ EXCEEDS LIMIT'\n",
    "print(f'  Status            = {safe}')\n",
    "print('═' * 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Setup Repo & Config ───────────────────────────────────────────────────────\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone your GitHub repo\n",
    "!git clone https://github.com/YOUR_USERNAME/vlm-temporal-logistics.git /kaggle/working/repo\n",
    "sys.path.insert(0, '/kaggle/working/repo')\n",
    "\n",
    "CONFIG_PATH = '/kaggle/working/repo/configs/training_config.yaml'\n",
    "\n",
    "# Update data paths for Kaggle\n",
    "import yaml\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['data']['openpack_root'] = '/kaggle/input/openpack/data'\n",
    "config['data']['shard_dir']     = '/kaggle/working/shards'\n",
    "config['training']['output_dir'] = '/kaggle/working/checkpoints'\n",
    "config['training']['report_to']  = 'none'  # Disable wandb on Kaggle\n",
    "\n",
    "kaggle_config_path = '/kaggle/working/config_kaggle.yaml'\n",
    "with open(kaggle_config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "print('Config updated for Kaggle paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Run Data Pipeline ─────────────────────────────────────────────────────────\n",
    "!python /kaggle/working/repo/data_pipeline.py \\\n",
    "    --config {kaggle_config_path} \\\n",
    "    --split train\n",
    "\n",
    "!python /kaggle/working/repo/data_pipeline.py \\\n",
    "    --config {kaggle_config_path} \\\n",
    "    --split val\n",
    "\n",
    "# Show shard files created\n",
    "!ls -lh /kaggle/working/shards/train/ | head -20\n",
    "!echo \"Shards created:\"\n",
    "!ls /kaggle/working/shards/train/*.tar | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Load Model + Apply QLoRA ──────────────────────────────────────────────────\n",
    "import torch, yaml\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "with open(kaggle_config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    config['model']['base_id'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# CRITICAL: Both flags required for quantized model + gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "lc = config['lora']\n",
    "lora_config = LoraConfig(\n",
    "    r=lc['r'],\n",
    "    lora_alpha=lc['lora_alpha'],\n",
    "    target_modules=lc['target_modules'],\n",
    "    lora_dropout=lc['lora_dropout'],\n",
    "    bias=lc['bias'],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Verify GPU memory after load\n",
    "alloc = torch.cuda.memory_allocated() / 1e9\n",
    "reserved = torch.cuda.memory_reserved() / 1e9\n",
    "print(f'\\nGPU memory allocated: {alloc:.2f} GB')\n",
    "print(f'GPU memory reserved:  {reserved:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Build Datasets + Start Training ──────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/repo')\n",
    "from training.finetune import (\n",
    "    OpenPackDataset, VLMCollator,\n",
    "    build_training_args\n",
    ")\n",
    "from transformers import AutoProcessor, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    config['model']['base_id'], trust_remote_code=True\n",
    ")\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "\n",
    "train_dataset = OpenPackDataset(\n",
    "    shard_dir='/kaggle/working/shards/train',\n",
    "    processor=processor,\n",
    "    frames_per_clip=config['data']['frames_per_clip'],\n",
    ")\n",
    "val_dataset = OpenPackDataset(\n",
    "    shard_dir='/kaggle/working/shards/val',\n",
    "    processor=processor,\n",
    "    frames_per_clip=config['data']['frames_per_clip'],\n",
    ")\n",
    "\n",
    "print(f'Train clips: {len(train_dataset)}')\n",
    "print(f'Val   clips: {len(val_dataset)}')\n",
    "\n",
    "training_args = build_training_args(config)\n",
    "collator      = VLMCollator(processor)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# Crash-tolerant: resume from latest checkpoint if exists\n",
    "from pathlib import Path\n",
    "checkpoints = sorted(Path(config['training']['output_dir']).glob('checkpoint-*'))\n",
    "checkpoint = str(checkpoints[-1]) if checkpoints else None\n",
    "if checkpoint:\n",
    "    print(f'Resuming from: {checkpoint}')\n",
    "\n",
    "trainer.train(resume_from_checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ── Save Final Model ──────────────────────────────────────────────────────────\n",
    "final_path = '/kaggle/working/checkpoints/final'\n",
    "trainer.save_model(final_path)\n",
    "processor.save_pretrained(final_path)\n",
    "print(f'Final model saved to: {final_path}')\n",
    "\n",
    "import json\n",
    "with open(f'{final_path}/training_log.json', 'w') as f:\n",
    "    json.dump(trainer.state.log_history, f, indent=2)\n",
    "\n",
    "# Final GPU memory check\n",
    "import torch\n",
    "alloc = torch.cuda.memory_allocated() / 1e9\n",
    "print(f'Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
